= Online Meetup: Building the Wikipedia Knowledge Graph in Neo4j

== Loading a reduced subset of the Wikipedia categories

Get data from https://www.mediawiki.org/wiki/API:Main_page[the  MediaWiki action API]

https://en.wikipedia.org/w/api.php?format=json&action=query&list=categorymembers&cmtype=subcat&cmtitle=Category:Databases&cmprop=ids%7Ctitle&cmlimit=500[sample request]

* STEP1: Prepare the DB

[source,cypher]
----
CREATE INDEX ON :Category(catId)
----

[source,cypher]
----
CREATE INDEX ON :Category(catName)
----

* STEP2: Select your root category

[source,cypher]
----
CREATE (c:Category:RootCategory {catId: 0, catName: 'Databases', fetched : false, level: 0 })
----

* STEP3: Load n levels by iteratively calling Wikipedia's rest API

[source,cypher]
----
UNWIND range(1,5) as level 
call apoc.cypher.doit("
MATCH (c:Category { fetched: false, level: $level - 1})
call apoc.load.json('https://en.wikipedia.org/w/api.php?format=json&action=query&list=categorymembers&cmtype=subcat&cmtitle=Category:' + apoc.text.urlencode(c.catName) + '&cmprop=ids%7Ctitle&cmlimit=500')
YIELD value as results
UNWIND results.query.categorymembers AS subcat
MERGE (sc:Category {catId: subcat.pageid})
ON CREATE SET sc.catName = substring(subcat.title,9),
              sc.fetched = false,
              sc.level = $level
WITH sc,c
CALL apoc.create.addLabels(sc,['Level' + $level + 'Category']) YIELD node
MERGE (sc)-[:SUBCAT_OF]->(c)
WITH DISTINCT c
SET c.fetched = true", { level: level }) yield value
return value
----


== Exploring the dataset

Biggest categories...

[source,cypher]
----
MATCH (c:Category)
RETURN c.catName AS category, 
	   size((c)<-[:SUBCAT_OF]-()) AS subcatCount, 
       size((c)-[:SUBCAT_OF]->()) AS superCatCount, 
       size((c)-[:SUBCAT_OF]-()) AS total
ORDER BY total DESC 
LIMIT 500
----

== Exploring the dataset

Adding a 'balance index'

[source,cypher]
----
MATCH (c:Category)
WITH c.catName AS category,        
	 size((c)<-[:SUBCAT_OF]-()) AS subCatCount, 
     size((c)-[:SUBCAT_OF]->()) AS superCatCount
WHERE  subCatCount > 0 AND superCatCount > 0
RETURN category, 
	   ABS(toFloat(superCatCount - subCatCount)/(superCatCount + subCatCount)) as balanceIndex, 
	   subCatCount, 
       superCatCount
ORDER BY balanceIndex DESC 
LIMIT 500
----

== Exploring the dataset

Aggregates...

[source,cypher]
----
MATCH (c:Category)
WITH c.catName AS category,        
	 size((c)<-[:SUBCAT_OF]-()) AS subCatCount, 
     size((c)-[:SUBCAT_OF]->()) AS superCatCount, 
     size((c)-[:SUBCAT_OF]-()) AS total
//WHERE  subCatCount > 0 AND superCatCount > 0
RETURN AVG(subCatCount) AS `AVG #subcats`,
	   MIN(subCatCount) AS `MIN #subcats`,
       MAX(subCatCount)  AS `MAX #subcats`, 
       percentileCont(subCatCount,0.9)  AS `.9p #subcats`,
       AVG(superCatCount) AS `AVG #supercats`,
       MIN(superCatCount) AS `MIN #supercats`,
       MAX(superCatCount) AS `MAX #supercats`, 
       percentileCont(superCatCount,0.95) AS `.9p #supercats`
----


== Exploring the dataset

What is the hierachy connecting DBs and Google Doodles?

[source,cypher]
----
MATCH shortestFullHierarchy = shortestPath((doodles:Category {catName : 'Google Doodles'})-[:SUBCAT_OF*]->(root:RootCategory)) 
RETURN shortestFullHierarchy
----

== Exploring the dataset

Some unexpectedly(?) long hierarchies

[source,cypher]
----
MATCH path =()-[r:SUBCAT_OF*10..]->() WITH path LIMIT 1
return path
----

== Exploring the dataset

Loops!

[source,cypher]
----
MATCH loop = (cat)-[r:SUBCAT_OF*]->(cat) 
RETURN loop LIMIT 1
----

== Loading the data from an sql dump via LOAD CSV

First load the categories

[source,cypher]
----
using periodic commit 10000
load csv from "https://github.com/jbarrasa/datasets/blob/master/wikipedia/data/cats.csv?raw=true" as row
CREATE (c:Category { catId: row[0]}) 
SET c.catName = row[2], c.pageCount = toInt(row[3]), c.subcatCount = toInt(row[4])
----

And then the subcategory relationships

[source,cypher]
----
using periodic commit 10000
load csv from "https://github.com/jbarrasa/datasets/blob/master/wikipedia/data/rels.csv?raw=true" as row
MATCH (from:Category { catId: row[0]}) 
MATCH (to:Category { catId: row[1]})
CREATE (from)-[:SUBCAT_OF]->(to)
----


== Loading the data from an sql dump via LOAD CSV

If you want to follow this approach, here's how you can generate the CSV files.

Get the latest dumps from: https://dumps.wikimedia.org/enwiki/latest/(the  wikipedia downloads page) and load them in a MySQL instance. You only need the following tables: category, categorylinks and page.

http://stackoverflow.com/questions/21782410/finding-subcategories-of-a-wikipedia-category-using-category-and-categorylinks-t

You can use my implementation->

Categories:
----
select p.page_id as PAGE_ID, c.cat_id as CAT_ID, cast(c.cat_title as nCHAR) as CAT_TITLE , c.cat_pages as CAT_PAGES_COUNT, c.cat_subcats as CAT_SUBCAT_COUNT
into outfile '/Users/jbarrasa/Applications/neo4j-enterprise-3.1.2/import/wiki/cats.csv' fields terminated by ',' enclosed by '"' escaped by '\\' lines terminated by '\n' 
from test.category c, test.page p
where c.cat_title = p.page_title
and p.page_namespace = 14
----

Subcategory relationships:
----
select p.page_id as FROM_PAGE_ID, p2.page_id as TO_PAGE_ID
into outfile '/Users/jbarrasa/Applications/neo4j-enterprise-3.1.2/import/wiki/rels.csv' fields terminated by ',' enclosed by '"' escaped by '\\' lines terminated by '\n' 
from test.category c, test.page p , test.categorylinks l, test.category c2, test.page p2
where l.cl_type = 'subcat'
	  and c.cat_title = p.page_title
      and p.page_namespace = 14
	  and l.cl_from = p.page_id
      and l.cl_to = c2.cat_title
      and c2.cat_title = p2.cat_title
      and p2.page_namespace = 14
----

== Exploring the dataset

Some interesting numbers (based on precomputed counts in wikipedia dump) 

[source,cypher]
----
MATCH (c:Category)
return SUM(c.pageCount) AS `#pages categorised (with duplicates)`,
	   AVG(c.pageCount) AS `average #pages per cat`, 
       percentileCont(c.pageCount, 0.75) AS `.75p #pages in a cat`,
	   MIN(c.pageCount) AS `min #pages in a cat`, 
       MAX(c.pageCount) AS `max #pages in a cat`
----	   


== Exploring the dataset

Orphan nodes? 

[source,cypher]
----
MATCH (c:Category)
WHERE NOT (c)-[:SUBCAT_OF]-()
RETURN COUNT(c)
----


== Exploring the dataset

Same analysis we did before...

[source,cypher]
----
MATCH (c:Category)
RETURN c.catName AS category, 
	   size((c)<-[:SUBCAT_OF]-()) AS subcatCount, 
       size((c)-[:SUBCAT_OF]->()) AS superCatCount, 
       size((c)-[:SUBCAT_OF]-()) AS total
ORDER BY total DESC 
LIMIT 500
----

== Exploring the dataset

Adding a 'balance index' ...and skipping the top ones

[source,cypher]
----
MATCH (c:Category)
WITH c.catName AS category,        
	 size((c)<-[:SUBCAT_OF]-()) AS subCatCount, 
     size((c)-[:SUBCAT_OF]->()) AS superCatCount
WHERE  subCatCount > 0 AND superCatCount > 0
RETURN category, 
	   ABS(toFloat(superCatCount - subCatCount)/(superCatCount + subCatCount)) as balanceIndex, 
	   subCatCount, 
       superCatCount
ORDER BY balanceIndex DESC 
SKIP 20000 LIMIT 500
----

== Exploring the dataset

Global aggregates

[source,cypher]
----
MATCH (c:Category)
WITH c.catName AS category,        
	 size((c)<-[:SUBCAT_OF]-()) AS subCatCount, 
     size((c)-[:SUBCAT_OF]->()) AS superCatCount, 
     size((c)-[:SUBCAT_OF]-()) AS total
//WHERE  subCatCount > 0 AND superCatCount > 0
RETURN AVG(subCatCount) AS `AVG #subcats`,
	   MIN(subCatCount) AS `MIN #subcats`,
       MAX(subCatCount)  AS `MAX #subcats`, 
       percentileCont(subCatCount,0.9)  AS `.9p #subcats`,
       AVG(superCatCount) AS `AVG #supercats`,
       MIN(superCatCount) AS `MIN #supercats`,
       MAX(superCatCount) AS `MAX #supercats`, 
       percentileCont(superCatCount,0.95) AS `.9p #supercats`
----

== Loading the data from an sql dump via APOC + JDBC

[source,cypher]
----
WITH "jdbc:mysql://localhost:3306/northwind?user=root&password=root" AS url,
     "select c.cat_id as CAT_ID, cast(c.cat_title as CHAR) as CAT_TITLE , c.cat_pages as CAT_PAGES_COUNT, c.cat_subcats as CAT_SUBCAT_COUNT
	  from test.category c, test.page p
	  where c.cat_title = p.page_title
 	  and p.page_namespace = 14 limit 300" AS sql
CALL apoc.load.jdbc(url,sql) YIELD row
CREATE (c:Category { catId: row.cat_id}) 
SET c.catName = row.CAT_TITLE, c.pageCount = toInt(row.cat_pages), c.subcatCount = toInt(row.cat_subcats)
----

[source,cypher]
----
WITH "jdbc:mysql://localhost:3306/northwind?user=root&password=root" AS url,
     "select p.page_id + 0 as FROM_PAGE_ID, p2.page_id + 0 as TO_PAGE_ID
from test.category c, test.page p , test.categorylinks l, test.category c2, test.page p2
where l.cl_type = 'subcat'
	  and c.cat_title = p.page_title
      and p.page_namespace = 14
	  and l.cl_from = p.page_id
      and l.cl_to = c2.cat_title
      and c2.cat_title = p2.page_title
      and p2.page_namespace = 14 
      limit 100 " AS sql
CALL apoc.load.jdbc(url,sql) YIELD row
MATCH (from:Category { catId: row.FROM_PAGE_ID}) 
MATCH (to:Category { catId: row.TO_PAGE_ID})
CREATE (from)-[:SUBCAT_OF]->(to)
----
